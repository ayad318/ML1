{
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Face Image Classification\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 1,
     "metadata": {},
     "outputs": [],
     "source": [
      "from os import listdir\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from random import randint\n",
      "from PIL import Image\n",
      "from sklearn import svm\n",
      "from sklearn.linear_model import LogisticRegression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data Organisation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Data Modeling\n",
      "\n",
      "NOTE: MAYBE WE SHOULD SEPERATE THIS INTO 2 SECTIONS: DATA MODELING AND \"HOW TO ORGANISE THE TRAINING SET AND TEST SET\" SO IT FOLLOWS THE TASK EVALUATION GUIDANCE A LITTLE CLOSER. \n",
      "\n",
      "To model the data, we will represent each image as an array mapping a greyscale value to each value. For this, we open each image and get the pixel greyscale values. We add this array and it's category to either the training set or the test set. The training set is the data we use to train our model, whereas the test set is the data we use to evaluate how well our model performs. We randomly decide which of the two sets each image is assigned to. This is the easiest way to ensure there is no bias regarding the training and test data, it's one of the easiest methodes to seperate the two sets while keeping the same proportion between categories, and we have enough data to get a functioning model using this method."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 2,
     "metadata": {},
     "outputs": [],
     "source": [
      "from sklearn.model_selection import train_test_split\n",
      "test_size = 0.33\n",
      "random_state = 0"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 3,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "Label  | % in all data | % in test data\n",
        "-------|---------------|---------------\n",
        "  1a   |     6.61%     |     6.32%\n",
        "  1b   |     6.09%     |     5.79%\n",
        "  1c   |     4.52%     |     4.74%\n",
        "  1d   |     4.17%     |     4.21%\n",
        "  1e   |     4.52%     |     4.74%\n",
        "  1f   |     4.00%     |     4.21%\n",
        "  1g   |     3.30%     |     3.16%\n",
        "  1h   |     3.83%     |     3.68%\n",
        "  1i   |     3.48%     |     3.68%\n",
        "  1j   |     5.57%     |     5.26%\n",
        "  1k   |     5.91%     |     5.79%\n",
        "  1l   |     5.91%     |     5.79%\n",
        "  1m   |     4.52%     |     4.74%\n",
        "  1n   |     5.22%     |     5.26%\n",
        "  1o   |     3.30%     |     3.16%\n",
        "  1p   |     4.52%     |     4.74%\n",
        "  1q   |     4.52%     |     4.74%\n",
        "  1r   |     5.74%     |     5.79%\n",
        "  1s   |     8.35%     |     8.42%\n",
        "  1t   |     5.91%     |     5.79%\n"
       ]
      }
     ],
     "source": [
      "# Convert images to vectors and store in x, y\n",
      "X, y = [], []\n",
      "for sample in listdir(\"cropped\"):\n",
      "    for pose in listdir(\"cropped/{}/face\".format(sample)):\n",
      "        X.append(np.array(Image.open(\"cropped/{}/face/{}\".format(sample, pose))).flatten())\n",
      "        y.append(sample)\n",
      "X = np.array(X, dtype=int)\n",
      "y = np.array(y, dtype=str)\n",
      "\n",
      "# Build Training and Testing Sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=test_size, random_state = random_state)\n",
      "\n",
      "# Verify that the data has been stratified correctly\n",
      "count_unique_labels_all = dict(zip(*np.unique(y, return_counts=True)))\n",
      "count_unique_labels_test = dict(zip(*np.unique(y_test, return_counts=True)))\n",
      "label_percentages = {k:[count_unique_labels_all[k]/len(y)*100, count_unique_labels_test[k]/len(y_test)*100] for k in count_unique_labels_all}\n",
      "print(\"Label  | % in all data | % in test data\")\n",
      "print(\"-------|---------------|---------------\")\n",
      "for k in label_percentages:\n",
      "    print(\"  {}   |     {:.2f}%     |     {:.2f}%\".format(k, label_percentages[k][0], label_percentages[k][1]))\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 89,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "(190, 10304) (190,) (385, 10304) (385,)\n"
       ]
      }
     ],
     "source": [
      "print(X_test.shape, y_test.shape, X_train.shape, y_train.shape)"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 90,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "['1f' '1b' '1p' '1b' '1n' '1s' '1s' '1s' '1q' '1s' '1s' '1s' '1q' '1s'\n",
        " '1r' '1a' '1o' '1i' '1d' '1l' '1i' '1a' '1n' '1j' '1n' '1k' '1c' '1h'\n",
        " '1p' '1f' '1p' '1l' '1l' '1c' '1a' '1t' '1j' '1t' '1s' '1r' '1k' '1b'\n",
        " '1t' '1m' '1m' '1f' '1k' '1r' '1q' '1o' '1n' '1e' '1e' '1h' '1t' '1s'\n",
        " '1s' '1f' '1r' '1k' '1c' '1m' '1e' '1h' '1a' '1l' '1i' '1c' '1p' '1k'\n",
        " '1r' '1k' '1b' '1d' '1a' '1s' '1b' '1b' '1l' '1r' '1a' '1l' '1n' '1e'\n",
        " '1a' '1r' '1q' '1r' '1t' '1l' '1c' '1q' '1m' '1h' '1h' '1c' '1n' '1s'\n",
        " '1a' '1d' '1t' '1n' '1r' '1j' '1m' '1q' '1k' '1d' '1o' '1a' '1t' '1l'\n",
        " '1s' '1h' '1b' '1k' '1h' '1g' '1h' '1e' '1b' '1a' '1a' '1a' '1f' '1m'\n",
        " '1n' '1o' '1s' '1i' '1b' '1e' '1e' '1f' '1c' '1b' '1l' '1b' '1r' '1m'\n",
        " '1j' '1k' '1t' '1a' '1m' '1l' '1s' '1i' '1t' '1j' '1c' '1d' '1b' '1m'\n",
        " '1o' '1p' '1n' '1p' '1o' '1s' '1c' '1p' '1g' '1e' '1s' '1d' '1t' '1q'\n",
        " '1q' '1q' '1t' '1l' '1a' '1b' '1q' '1n' '1i' '1l' '1s' '1t' '1h' '1s'\n",
        " '1n' '1t' '1p' '1t' '1s' '1r' '1l' '1j' '1i' '1k' '1l' '1h' '1q' '1a'\n",
        " '1c' '1b' '1j' '1k' '1o' '1l' '1p' '1g' '1b' '1p' '1r' '1a' '1q' '1j'\n",
        " '1t' '1l' '1e' '1c' '1e' '1m' '1b' '1g' '1j' '1j' '1t' '1t' '1p' '1a'\n",
        " '1f' '1d' '1n' '1r' '1g' '1o' '1e' '1f' '1o' '1a' '1a' '1r' '1k' '1o'\n",
        " '1k' '1p' '1i' '1b' '1q' '1b' '1l' '1a' '1r' '1c' '1m' '1e' '1q' '1s'\n",
        " '1b' '1s' '1p' '1e' '1d' '1b' '1k' '1r' '1o' '1g' '1d' '1b' '1p' '1o'\n",
        " '1n' '1i' '1d' '1c' '1f' '1c' '1s' '1g' '1e' '1r' '1g' '1a' '1j' '1g'\n",
        " '1b' '1m' '1h' '1n' '1s' '1s' '1j' '1c' '1j' '1h' '1i' '1q' '1a' '1k'\n",
        " '1r' '1l' '1t' '1n' '1s' '1j' '1k' '1b' '1h' '1q' '1k' '1d' '1d' '1d'\n",
        " '1k' '1s' '1g' '1i' '1t' '1a' '1p' '1n' '1n' '1n' '1m' '1b' '1d' '1j'\n",
        " '1k' '1s' '1f' '1s' '1f' '1e' '1t' '1g' '1p' '1i' '1d' '1l' '1j' '1j'\n",
        " '1m' '1k' '1a' '1p' '1k' '1t' '1g' '1k' '1i' '1l' '1f' '1h' '1g' '1c'\n",
        " '1s' '1m' '1m' '1o' '1j' '1t' '1e' '1j' '1k' '1l' '1f' '1n' '1l' '1m'\n",
        " '1s' '1s' '1f' '1r' '1t' '1r' '1l' '1d' '1r' '1j' '1j' '1a' '1n' '1j'\n",
        " '1a' '1e' '1r' '1h' '1f' '1q' '1c']\n"
       ]
      }
     ],
     "source": [
      "print(y_train)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Training, Testing and Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Support Vector Machines"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "clf = svm.SVC(kernel='linear').fit(X_train, y_train)"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "0.9842105263157894\n"
       ]
      }
     ],
     "source": [
      "print(clf.score(X_test, y_test))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Logistic Regression\n",
      "\n",
      "Logistic Regression is a way of classifying data using the sigmoid function \n",
      "$$g(z) = \\frac{1}{1+e^{-z}}$$"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 96,
     "metadata": {},
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": [
        "/Users/ayadfarhat/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
        "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
        "\n",
        "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
        "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "Please also refer to the documentation for alternative solver options:\n",
        "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
        "  n_iter_i = _check_optimize_result(\n"
       ]
      },
      {
       "data": {
        "text/plain": [
         "LogisticRegression(multi_class='ovr')"
        ]
       },
       "execution_count": 96,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "source": [
      "log_reg = LogisticRegression(multi_class=\"ovr\")\n",
      "log_reg.fit(X_train, y_train)"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 97,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "0.9789473684210527\n"
       ]
      }
     ],
     "source": [
      "print(log_reg.score(X_test, y_test))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Evaluation\n",
      "\n",
      "1) Performance evalution tell you if you're making progress, and put a number on it. All machine learning models, whether it's linear regression, or else, need a metric to judge performance. This is done by using some of the copmmon evaulation measures to get data output that reflects on the performance of the model"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 24,
     "metadata": {},
     "outputs": [],
     "source": [
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import auc"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 128,
     "metadata": {},
     "outputs": [],
     "source": [
      "y_pred = log_reg.predict(X_test)\n",
      "precision = precision_score(y_test, y_pred, average='macro')\n",
      "recall = recall_score(y_test, y_pred, average='macro')\n",
      "f1 = 2*precision*recall/(precision + recall)\n",
      "accuracy = accuracy_score(y_test, y_pred)\n",
      "cm = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "\n",
      "fp = cm.sum(axis=0) - np.diag(cm)  \n",
      "fn = cm.sum(axis=1) - np.diag(cm)\n",
      "tp = np.diag(cm)\n",
      "tn = cm.sum() - (fp + fn + tp)\n",
      "specificity = tn/(tn+fp)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2)\n",
      "TP: True positives are the cases when the actual class of the data point was 1(True) and the predicted is also 1(True)\n",
      "\n",
      "FP: False positives are the cases when the actual class of the data point was 0(False) and the predicted is 1(True). False is because the model has predicted incorrectly and positive because the class predicted was a positive one. (1)\n",
      "\n",
      "TN: True negatives are the cases when the actual class of the data point was 0(False) and the predicted is also 0(False)\n",
      "\n",
      "FN: False negatives are the cases when the actual class of the data point was 1(True) and the predicted is 0(False). False is because the model has predicted incorrectly and negative because the class predicted was a negative one. (0)"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 129,
     "metadata": {
      "scrolled": true
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "TP: [11 11  9  8  9  8  6  6  7  9 11 10  9 10  6  9  9 11 16 11]\n",
        "FP: [0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0]\n",
        "TN: [178 179 181 182 180 181 184 183 183 179 179 179 181 180 184 180 181 179\n",
        " 174 179]\n",
        "FN: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0]\n"
       ]
      }
     ],
     "source": [
      "print(\"TP: \" + str(tp))\n",
      "print(\"FP: \" + str(fp))\n",
      "print(\"TN: \" + str(tn))\n",
      "print(\"FN: \" + str(fn))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3)\n",
      "Accuracy using crossvalidation: the measurement used to determine which model is best at identifying relationships and patterns between variables in a dataset based on the input, or training, data.\n",
      "\n",
      "Precision: Precision is one indicator of a machine learning model's performance – the quality of a positive prediction made by the model. Precision refers to the number of true positives divided by the total number of positive predictions \n",
      "\n",
      "Sensitivity: Sensitivity is a measure of how well a machine learning model can detect positive instances. It is also known as the true positive rate (TPR) or recall. \n",
      "\n",
      "Specificity: is the proportion of true negatives that are correctly predicted by the model.\n",
      "\n",
      "F1-Score: It elegantly sums up the predictive performance of a model by combining two otherwise competing metrics — precision and recall.\n",
      "\n",
      "Confusion Matrix: Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model\n",
      "\n",
      "AUC: the Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.\n",
      "\n",
      "ROC: ROC curve, also known as Receiver Operating Characteristics Curve, is a metric used to measure the performance of a classifier model. The ROC curve depicts the rate of true positives with respect to the rate of false positives, therefore highlighting the sensitivity of the classifier model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "4)\n",
      "\n",
      "Accuracy:"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 130,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "0.9789473684210527\n"
       ]
      }
     ],
     "source": [
      "print(accuracy)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Precision:"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 131,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "0.9794444444444445\n"
       ]
      }
     ],
     "source": [
      "print(precision)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sensitivity:"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 132,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "0.9791450216450217\n"
       ]
      }
     ],
     "source": [
      "print(recall)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Specificity"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 133,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "0.994475138121547\n"
       ]
      }
     ],
     "source": [
      "print(specificity[4])"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "F1-Score"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 134,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "0.9792947101573404\n"
       ]
      }
     ],
     "source": [
      "print(f1)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Confusion Matrix"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 135,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "[[11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0]\n",
        " [ 0 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
        " [ 0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
        " [ 0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
        " [ 0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
        " [ 0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
        " [ 0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
        " [ 0  0  0  0  1  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0]\n",
        " [ 0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0]\n",
        " [ 0  0  0  0  0  1  0  0  0  9  0  0  0  0  0  0  0  0  0  0]\n",
        " [ 0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0]\n",
        " [ 0  0  0  0  0  0  0  0  0  1  0 10  0  0  0  0  0  0  0  0]\n",
        " [ 0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0]\n",
        " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0]\n",
        " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0]\n",
        " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0]\n",
        " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0]\n",
        " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0  0]\n",
        " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16  0]\n",
        " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11]]\n"
       ]
      }
     ],
     "source": [
      "print(cm)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "AUC, ROC:"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 1,
     "metadata": {},
     "outputs": [],
     "source": [
      "fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=2)\n",
      "auc=auc(fpr, tpr)\n",
      "print(auc)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "5) \n",
      "Accuracy: to more accurate model outcomes result in better decisions. \n",
      "\n",
      "Precision: Models inherently trade off between precision and recall. Typically, the higher the precision, the lower the recall, and vice versa.\n",
      "\n",
      "Sensitivity: to evaluate model performance because it allows us to see how many positive instances the model was able to correctly identify.\n",
      "\n",
      "Specificty: to determine the proportion of actual negative cases, which got predicted correctly.\n",
      "\n",
      "F1-Score: to sums up the predictive performance of a model by combining two otherwise competing metrics.\n",
      "\n",
      "Confusion Matrix: helps you to the know the performance of the classification model on a set of test data for that the true values are known.\n",
      "\n",
      "AUC: as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.\n",
      "\n",
      "ROC: to select a threshold for a classifier, which maximizes the true positives and in turn minimizes the false positives"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Improvement"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## PCA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Import PCA class from sklearn"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 28,
     "metadata": {},
     "outputs": [],
     "source": [
      "from sklearn.decomposition import PCA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perform PCA dimensionality reduction on training and testing data."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 58,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "10\n"
       ]
      }
     ],
     "source": [
      "pca = PCA(n_components=10)\n",
      "pca.fit_transform(X_train)\n",
      "pca_X_train = pca.transform(X_train)\n",
      "pca_X_test = pca.transform(X_test)\n",
      "print(pca.n_components_)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perform logistic regression with the new data and print accuracy."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 61,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "0.8842105263157894\n"
       ]
      }
     ],
     "source": [
      "logreg = LogisticRegression(max_iter=800)\n",
      "logreg.fit(pca_X_train, y_train)\n",
      "y_pred = logreg.predict(pca_X_test)\n",
      "print(accuracy_score(y_test, y_pred))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perform logistic regression using the original data and print accuracy."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 39,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "0.9789473684210527\n"
       ]
      }
     ],
     "source": [
      "logreg = LogisticRegression(max_iter=200)\n",
      "logreg.fit(X_train, y_train)\n",
      "y_pred = logreg.predict(X_test)\n",
      "print(accuracy_score(y_test, y_pred))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## LDA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Import LDA class from sklearn"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 41,
     "metadata": {},
     "outputs": [],
     "source": [
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perform LDA dimensionality reduction on training and testing data."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 57,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "10\n"
       ]
      }
     ],
     "source": [
      "lda = LinearDiscriminantAnalysis(n_components=10)\n",
      "lda_X_train = lda.fit_transform(X_train, y_train)\n",
      "lda_X_test = lda.transform(X_test)\n",
      "print(lda.n_components)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perform logistic regression using the new data and print accuracy."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 53,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "0.9736842105263158\n"
       ]
      }
     ],
     "source": [
      "logreg = LogisticRegression(max_iter=2000)\n",
      "logreg.fit(lda_X_train, y_train)\n",
      "y_pred = logreg.predict(lda_X_test)\n",
      "print(accuracy_score(y_test, y_pred))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perform logistic regression using the original data and print accuracy."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "0.9789473684210527\n"
       ]
      }
     ],
     "source": [
      "logreg = LogisticRegression(max_iter=200)\n",
      "logreg.fit(X_train, y_train)\n",
      "y_pred = logreg.predict(X_test)\n",
      "print(accuracy_score(y_test, y_pred))"
     ]
    }
   ],
   "metadata": {
    "kernelspec": {
     "display_name": "Python 3 (ipykernel)",
     "language": "python",
     "name": "python3"
    },
    "language_info": {
     "codemirror_mode": {
      "name": "ipython",
      "version": 3
     },
     "file_extension": ".py",
     "mimetype": "text/x-python",
     "name": "python",
     "nbconvert_exporter": "python",
     "pygments_lexer": "ipython3",
     "version": "3.8.8"
    }
   },
   "nbformat": 4,
   "nbformat_minor": 2
  }