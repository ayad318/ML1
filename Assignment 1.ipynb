{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Image Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from PIL import Image\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling\n",
    "\n",
    "To model the data, we will represent each image as an array mapping a greyscale value to each value. For this, we open each image and get the pixel greyscale values. We add this array and it's category to either the training set or the test set. The training set is the data we use to train our model, whereas the test set is the data we use to evaluate how well our model performs. We randomly decide which of the two sets each image is assigned to. This is the easiest way to ensure there is no bias regarding the training and test data, it's one of the easiest methodes to seperate the two sets while keeping the same proportion between categories, and we have enough data to get a functioning model using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_size = 0.33\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label  | % in all data | % in test data\n",
      "-------|---------------|---------------\n",
      "  1a   |     6.61%     |     6.32%\n",
      "  1b   |     6.09%     |     5.79%\n",
      "  1c   |     4.52%     |     4.74%\n",
      "  1d   |     4.17%     |     4.21%\n",
      "  1e   |     4.52%     |     4.74%\n",
      "  1f   |     4.00%     |     4.21%\n",
      "  1g   |     3.30%     |     3.16%\n",
      "  1h   |     3.83%     |     3.68%\n",
      "  1i   |     3.48%     |     3.68%\n",
      "  1j   |     5.57%     |     5.26%\n",
      "  1k   |     5.91%     |     5.79%\n",
      "  1l   |     5.91%     |     5.79%\n",
      "  1m   |     4.52%     |     4.74%\n",
      "  1n   |     5.22%     |     5.26%\n",
      "  1o   |     3.30%     |     3.16%\n",
      "  1p   |     4.52%     |     4.74%\n",
      "  1q   |     4.52%     |     4.74%\n",
      "  1r   |     5.74%     |     5.79%\n",
      "  1s   |     8.35%     |     8.42%\n",
      "  1t   |     5.91%     |     5.79%\n"
     ]
    }
   ],
   "source": [
    "# Convert images to vectors and store in x, y\n",
    "X, y = [], []\n",
    "for sample in listdir(\"cropped\"):\n",
    "    for pose in listdir(\"cropped/{}/face\".format(sample)):\n",
    "        X.append(np.array(Image.open(\"cropped/{}/face/{}\".format(sample, pose))).flatten())\n",
    "        y.append(sample)\n",
    "X = np.array(X, dtype=int)\n",
    "y = np.array(y, dtype=str)\n",
    "\n",
    "# Build Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=test_size, random_state = random_state)\n",
    "\n",
    "# Verify that the data has been stratified correctly\n",
    "count_unique_labels_all = dict(zip(*np.unique(y, return_counts=True)))\n",
    "count_unique_labels_test = dict(zip(*np.unique(y_test, return_counts=True)))\n",
    "label_percentages = {k:[count_unique_labels_all[k]/len(y)*100, count_unique_labels_test[k]/len(y_test)*100] for k in count_unique_labels_all}\n",
    "print(\"Label  | % in all data | % in test data\")\n",
    "print(\"-------|---------------|---------------\")\n",
    "for k in label_percentages:\n",
    "    print(\"  {}   |     {:.2f}%     |     {:.2f}%\".format(k, label_percentages[k][0], label_percentages[k][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190, 10304) (190,) (385, 10304) (385,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, y_test.shape, X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1f' '1b' '1p' '1b' '1n' '1s' '1s' '1s' '1q' '1s' '1s' '1s' '1q' '1s'\n",
      " '1r' '1a' '1o' '1i' '1d' '1l' '1i' '1a' '1n' '1j' '1n' '1k' '1c' '1h'\n",
      " '1p' '1f' '1p' '1l' '1l' '1c' '1a' '1t' '1j' '1t' '1s' '1r' '1k' '1b'\n",
      " '1t' '1m' '1m' '1f' '1k' '1r' '1q' '1o' '1n' '1e' '1e' '1h' '1t' '1s'\n",
      " '1s' '1f' '1r' '1k' '1c' '1m' '1e' '1h' '1a' '1l' '1i' '1c' '1p' '1k'\n",
      " '1r' '1k' '1b' '1d' '1a' '1s' '1b' '1b' '1l' '1r' '1a' '1l' '1n' '1e'\n",
      " '1a' '1r' '1q' '1r' '1t' '1l' '1c' '1q' '1m' '1h' '1h' '1c' '1n' '1s'\n",
      " '1a' '1d' '1t' '1n' '1r' '1j' '1m' '1q' '1k' '1d' '1o' '1a' '1t' '1l'\n",
      " '1s' '1h' '1b' '1k' '1h' '1g' '1h' '1e' '1b' '1a' '1a' '1a' '1f' '1m'\n",
      " '1n' '1o' '1s' '1i' '1b' '1e' '1e' '1f' '1c' '1b' '1l' '1b' '1r' '1m'\n",
      " '1j' '1k' '1t' '1a' '1m' '1l' '1s' '1i' '1t' '1j' '1c' '1d' '1b' '1m'\n",
      " '1o' '1p' '1n' '1p' '1o' '1s' '1c' '1p' '1g' '1e' '1s' '1d' '1t' '1q'\n",
      " '1q' '1q' '1t' '1l' '1a' '1b' '1q' '1n' '1i' '1l' '1s' '1t' '1h' '1s'\n",
      " '1n' '1t' '1p' '1t' '1s' '1r' '1l' '1j' '1i' '1k' '1l' '1h' '1q' '1a'\n",
      " '1c' '1b' '1j' '1k' '1o' '1l' '1p' '1g' '1b' '1p' '1r' '1a' '1q' '1j'\n",
      " '1t' '1l' '1e' '1c' '1e' '1m' '1b' '1g' '1j' '1j' '1t' '1t' '1p' '1a'\n",
      " '1f' '1d' '1n' '1r' '1g' '1o' '1e' '1f' '1o' '1a' '1a' '1r' '1k' '1o'\n",
      " '1k' '1p' '1i' '1b' '1q' '1b' '1l' '1a' '1r' '1c' '1m' '1e' '1q' '1s'\n",
      " '1b' '1s' '1p' '1e' '1d' '1b' '1k' '1r' '1o' '1g' '1d' '1b' '1p' '1o'\n",
      " '1n' '1i' '1d' '1c' '1f' '1c' '1s' '1g' '1e' '1r' '1g' '1a' '1j' '1g'\n",
      " '1b' '1m' '1h' '1n' '1s' '1s' '1j' '1c' '1j' '1h' '1i' '1q' '1a' '1k'\n",
      " '1r' '1l' '1t' '1n' '1s' '1j' '1k' '1b' '1h' '1q' '1k' '1d' '1d' '1d'\n",
      " '1k' '1s' '1g' '1i' '1t' '1a' '1p' '1n' '1n' '1n' '1m' '1b' '1d' '1j'\n",
      " '1k' '1s' '1f' '1s' '1f' '1e' '1t' '1g' '1p' '1i' '1d' '1l' '1j' '1j'\n",
      " '1m' '1k' '1a' '1p' '1k' '1t' '1g' '1k' '1i' '1l' '1f' '1h' '1g' '1c'\n",
      " '1s' '1m' '1m' '1o' '1j' '1t' '1e' '1j' '1k' '1l' '1f' '1n' '1l' '1m'\n",
      " '1s' '1s' '1f' '1r' '1t' '1r' '1l' '1d' '1r' '1j' '1j' '1a' '1n' '1j'\n",
      " '1a' '1e' '1r' '1h' '1f' '1q' '1c']\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression is a way of classifying data using the sigmoid function \n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='ovr')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(multi_class=\"ovr\")\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9789473684210527\n"
     ]
    }
   ],
   "source": [
    "print(log_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9842105263157894\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9817424242424242\n",
      "0.9753571428571428\n",
      "0.9785393671614059\n",
      "[[11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  9  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_reg.predict(X_test)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = 2*precision*recall/(precision + recall)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
